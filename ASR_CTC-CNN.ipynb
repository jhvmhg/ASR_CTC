{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "################################################################################\n",
      "### WARNING, path does not exist: KALDI_ROOT=/mnt/matylda5/iveselyk/Tools/kaldi-trunk\n",
      "###          (please add 'export KALDI_ROOT=<your_path>' in your $HOME/.profile)\n",
      "###          (or run as: KALDI_ROOT=<your_path> python <your_script>.py)\n",
      "################################################################################\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from pathlib import Path\n",
    "import kaldiio\n",
    "import sys\n",
    "import gc\n",
    "import json\n",
    "import time\n",
    "from data_4 import AudioDataLoader, AudioDataset, pad_list\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "print_use = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_json = \"/home1/meichaoyang/workspace/git/espnet/egs/aishell2/asr1/dump/train_sp/deltafalse/data.json\"\n",
    "test_json = \"/home1/meichaoyang/workspace/git/espnet/egs/aishell2/asr1/dump/test/deltafalse/data.json\"\n",
    "batch_size = 32\n",
    "maxlen_in = 1000\n",
    "maxlen_out = 50\n",
    "num_workers = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\"\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_dataset = AudioDataset(train_json, batch_size,\n",
    "                              maxlen_in, maxlen_out)\n",
    "tr_loader = AudioDataLoader(tr_dataset, batch_size=1, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "te_dataset = AudioDataset(test_json, batch_size,\n",
    "                              maxlen_in, maxlen_out)\n",
    "te_loader = AudioDataLoader(te_dataset, batch_size=1, num_workers=num_workers, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_list = []\n",
    "char_list_path = \"/home1/meichaoyang/workspace/git/espnet/egs/aishell2/asr3/data/lang_1char/train_units.txt\"\n",
    "with open(char_list_path, \"r\") as f:\n",
    "    for line in f:\n",
    "        data = line.split()\n",
    "        char_list.append(data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型搭建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, rnn_hidden_size, vocab_size, bidirectional=False, dropout=0.0):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.rnn_hidden_size = rnn_hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        self.cnn1 = nn.Conv2d(1, 128, (5, 3), stride=(2, 1), padding=(2, 1))\n",
    "        \n",
    "        self.cnn1_out_shape_h = (input_size+2*4-5)//2 + 1\n",
    "#         self.cnn1_out_shape_w = (input_size.shape[1]+2*2-3)/1 + 1\n",
    "        self.mlp1 = nn.Sequential(\n",
    "            nn.Linear(128 * input_size,\n",
    "                      self.rnn_hidden_size*2),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(self.rnn_hidden_size*2, self.rnn_hidden_size))\n",
    "        \n",
    "        self.rnn = nn.LSTM(self.rnn_hidden_size, self.rnn_hidden_size, \n",
    "                           batch_first=True,\n",
    "                           dropout=dropout,\n",
    "                           bidirectional=bidirectional)\n",
    "        self.mlp2 = nn.Sequential(\n",
    "            nn.Linear(self.rnn_hidden_size*2,\n",
    "                      self.rnn_hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(self.rnn_hidden_size, self.vocab_size))\n",
    "\n",
    "\n",
    "    def forward(self, padded_input, input_lengths):\n",
    "        # padded_input: N * T * D\n",
    "        # input_lengths: N\n",
    "\n",
    "        # padded_input: torch.Size([10, 2145, 83])\n",
    "        # padded_input1: torch.Size([10, 1, 2145, 83])\n",
    "        # cnn1_output: torch.Size([10, 128, 1073, 83])\n",
    "        # mlp1_input: torch.Size([10, 1073, 128, 83])\n",
    "\n",
    "\n",
    "        N = padded_input.shape[0]\n",
    "#         print(\"padded_input:\",padded_input.shape)\n",
    "        padded_input1 = padded_input.unsqueeze(1) \n",
    "#         print(\"padded_input1:\",padded_input1.shape)\n",
    "        cnn1_output = self.cnn1(padded_input1)\n",
    "#         print(\"cnn1_output:\",cnn1_output.shape)\n",
    "        \n",
    "        mlp1_input = torch.transpose(cnn1_output, 1,2)\n",
    "#         print(\"mlp1_input:\",mlp1_input.shape)\n",
    "        mlp1_input = mlp1_input.reshape((N,-1,128*83))\n",
    "        \n",
    "        mlp1_output = self.mlp1(mlp1_input)\n",
    "#         print(\"mlp1_output:\",mlp1_output.shape)\n",
    "        \n",
    "#         print(\"input_lengths:\",input_lengths)\n",
    "        input_lengths = input_lengths//2\n",
    "        total_length = padded_input.size(1)  # get the max sequence length\n",
    "        packed_input = pack_padded_sequence(mlp1_output, input_lengths,\n",
    "                                            batch_first=True)\n",
    "#         print(\"packed_input:\",packed_input.shape)\n",
    "        packed_output, hidden = self.rnn(packed_input)\n",
    "        rnn_output, _ = pad_packed_sequence(packed_output,\n",
    "                                        batch_first=True,\n",
    "                                        total_length=total_length)\n",
    "        predicted_y = self.mlp2(rnn_output)\n",
    "        return predicted_y\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CTC_Model(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, rnn_hidden_size, vocab_size, bidirectional=True, dropout=0.0):\n",
    "        super(CTC_Model, self).__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.encoder = Encoder(input_size, rnn_hidden_size, vocab_size, bidirectional, dropout)\n",
    "        self.ctc_loss = nn.CTCLoss()\n",
    "    \n",
    "    def forward(self, padded_input, input_lengths, padded_target, target_lengths):\n",
    "        \n",
    "        encoder_output = self.encoder(padded_input, input_lengths)\n",
    "        encoder_output = torch.transpose(encoder_output,0,1)\n",
    "        ctc_input = encoder_output.log_softmax(2)\n",
    "        \n",
    "        loss = self.ctc_loss(ctc_input, padded_target, input_lengths, target_lengths)\n",
    "        \n",
    "        return loss\n",
    "        \n",
    "    def recognize(self, input, input_lengths, char_list):\n",
    "        \"\"\"Sequence-to-Sequence beam search, decode one utterence now.\n",
    "        Args:\n",
    "            input: T x D\n",
    "            char_list: list of characters\n",
    "            args: args.beam\n",
    "            padded_input: N * T * F_dim\n",
    "            input_lengths: N *\n",
    "\n",
    "        Returns:\n",
    "            nbest_hyps:\n",
    "        \"\"\"\n",
    "        encoder_output = self.encoder(input, input_lengths)\n",
    "        ans=torch.max(encoder_output,-1)\n",
    "        res = \"\"\n",
    "        for j in range(ans[1].shape[1]):\n",
    "            v = ans[1][0][j].item()\n",
    "            if v>0:\n",
    "                res=res+char_list[v]\n",
    "\n",
    "        return res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(model, epoch, optimizier, print_every=10, plot_every=10, learning_rate=0.01):\n",
    "\n",
    "    start = time.time()\n",
    "    n_iters = len(tr_dataset)\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    \n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for e in range(epoch):\n",
    "        for i, (data) in enumerate(tr_loader):\n",
    "            padded_input, input_lengths, padded_target, target_lengths= data\n",
    "            padded_input = padded_input.to(device)\n",
    "            input_lengths = input_lengths.to(device)\n",
    "            padded_target = padded_target.to(device)\n",
    "            target_lengths = target_lengths.to(device)\n",
    "            loss = model(padded_input, input_lengths, padded_target, target_lengths)\n",
    "    #         print(loss) #.requires_grad\n",
    "            print_loss_total += float(loss)\n",
    "            plot_loss_total += float(loss)\n",
    "\n",
    "            optimizier.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            optimizier.step()\n",
    "\n",
    "            if (i+1) % print_every == 0:\n",
    "                print_loss_avg = print_loss_total / print_every\n",
    "                print_loss_total = 0\n",
    "                txt = 'Epoch %d | Iter %d | %s (%d %d%%) %.4f' % (e+1, i+1, timeSince(start, (e *n_iters +i+1) / (n_iters*epoch)),\n",
    "                                             (i+1), (e *n_iters +i+1) / (n_iters*epoch) * 100, print_loss_avg)\n",
    "                print(txt)\n",
    "\n",
    "            if i+1 % plot_every == 0:\n",
    "                plot_loss_avg = plot_loss_total / plot_every\n",
    "                plot_losses.append(plot_loss_avg)\n",
    "                plot_loss_total = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CTC_Model(\n",
      "  (encoder): Encoder(\n",
      "    (cnn1): Conv2d(1, 128, kernel_size=(5, 3), stride=(2, 1), padding=(2, 1))\n",
      "    (mlp1): Sequential(\n",
      "      (0): Linear(in_features=10624, out_features=512, bias=True)\n",
      "      (1): Tanh()\n",
      "      (2): Linear(in_features=512, out_features=256, bias=True)\n",
      "    )\n",
      "    (rnn): LSTM(256, 256, batch_first=True, bidirectional=True)\n",
      "    (mlp2): Sequential(\n",
      "      (0): Linear(in_features=512, out_features=256, bias=True)\n",
      "      (1): Tanh()\n",
      "      (2): Linear(in_features=256, out_features=6039, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (ctc_loss): CTCLoss()\n",
      ")\n",
      "Epoch 1 | Iter 100 | 0m 17s (- 4992m 19s) (100 0%) 222.5126\n",
      "Epoch 1 | Iter 200 | 0m 30s (- 4501m 0s) (200 0%) 45.2970\n",
      "Epoch 1 | Iter 300 | 0m 44s (- 4358m 44s) (300 0%) 7.5657\n",
      "Epoch 1 | Iter 400 | 1m 6s (- 4837m 17s) (400 0%) 7.2154\n",
      "Epoch 1 | Iter 500 | 1m 26s (- 5083m 28s) (500 0%) 7.1020\n",
      "Epoch 1 | Iter 600 | 1m 47s (- 5230m 41s) (600 0%) 7.0447\n",
      "Epoch 1 | Iter 700 | 2m 7s (- 5333m 20s) (700 0%) 7.0112\n",
      "Epoch 1 | Iter 800 | 2m 28s (- 5440m 24s) (800 0%) 6.9840\n",
      "Epoch 1 | Iter 900 | 2m 50s (- 5547m 39s) (900 0%) 6.9691\n"
     ]
    }
   ],
   "source": [
    "input_size = 83\n",
    "\n",
    "hidden_size = 256\n",
    "vocab_size = len(char_list)\n",
    "embedding_dim = 512\n",
    "sos_id = 0\n",
    "eos_id = 1\n",
    "learning_rate = 1e-3\n",
    "momentum = 0\n",
    "l2 = 1e-5\n",
    "\n",
    "IGNORE_ID=-1\n",
    "\n",
    "\n",
    "model = CTC_Model(input_size, hidden_size,vocab_size)\n",
    "print(model)\n",
    "model.to(device)\n",
    "\n",
    "optimizier = torch.optim.Adam(model.parameters(),\n",
    "                                     lr=learning_rate,\n",
    "#                                      momentum=momentum,\n",
    "                                     weight_decay=l2)\n",
    "trainIters(model, 20,optimizier, print_every=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}