{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "################################################################################\n",
      "### WARNING, path does not exist: KALDI_ROOT=/mnt/matylda5/iveselyk/Tools/kaldi-trunk\n",
      "###          (please add 'export KALDI_ROOT=<your_path>' in your $HOME/.profile)\n",
      "###          (or run as: KALDI_ROOT=<your_path> python <your_script>.py)\n",
      "################################################################################\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "\n",
    "import torch\n",
    "# import torchaudio\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from pathlib import Path\n",
    "import kaldi_io\n",
    "import sys\n",
    "import gc\n",
    "import json\n",
    "import time\n",
    "from data_4 import AudioDataLoader, AudioDataset, pad_list\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "print_use = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_json = \"/home1/meichaoyang/workspace/git/espnet/egs/aishell2/asr1/dump/train_sp/deltafalse/data.json\"\n",
    "test_json = \"/home1/meichaoyang/workspace/git/espnet/egs/aishell2/asr1/dump/test/deltafalse/data.json\"\n",
    "batch_size = 32\n",
    "maxlen_in = 1000\n",
    "maxlen_out = 50\n",
    "num_workers = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\"\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_dataset = AudioDataset(train_json, batch_size,\n",
    "                              maxlen_in, maxlen_out)\n",
    "tr_loader = AudioDataLoader(tr_dataset, batch_size=1, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "te_dataset = AudioDataset(test_json, batch_size,\n",
    "                              maxlen_in, maxlen_out)\n",
    "te_loader = AudioDataLoader(te_dataset, batch_size=1, num_workers=num_workers, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_list = []\n",
    "char_list_path = \"/home1/meichaoyang/workspace/git/espnet/egs/aishell2/asr3/data/lang_1char/train_units.txt\"\n",
    "with open(char_list_path, \"r\") as f:\n",
    "    for line in f:\n",
    "        data = line.split()\n",
    "        char_list.append(data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型搭建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, bidirectional=True, dropout=0.0):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.first = True\n",
    "        self.rnn = nn.LSTM(input_size, hidden_size, \n",
    "                           batch_first=True,\n",
    "                           dropout=dropout,\n",
    "                           bidirectional=bidirectional)\n",
    "\n",
    "\n",
    "    def forward(self, padded_input, input_lengths):\n",
    "        \n",
    "        total_length = padded_input.size(1)  # get the max sequence length\n",
    "        packed_input = pack_padded_sequence(padded_input, input_lengths,\n",
    "                                            batch_first=True)\n",
    "        packed_output, hidden = self.rnn(packed_input)\n",
    "        output, _ = pad_packed_sequence(packed_output,\n",
    "                                        batch_first=True,\n",
    "                                        total_length=total_length)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CTC_Model(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, bidirectional=True, dropout=0.0):\n",
    "        super(CTC_Model, self).__init__()\n",
    "        self.encoder = Encoder(input_size, hidden_size,bidirectional, dropout)\n",
    "        self.ctc_loss = nn.CTCLoss()\n",
    "    \n",
    "    def forward(self, padded_input, input_lengths, padded_target, target_lengths):\n",
    "        \n",
    "        encoder_output, _ = self.encoder(padded_input, input_lengths)\n",
    "        encoder_output = torch.transpose(encoder_output,0,1)\n",
    "        ctc_input = encoder_output.log_softmax(2)\n",
    "        \n",
    "        loss = self.ctc_loss(ctc_input, padded_target, input_lengths, target_lengths)\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(model, epoch, optimizier, print_every=10, plot_every=10, learning_rate=0.01):\n",
    "\n",
    "    start = time.time()\n",
    "    n_iters = len(tr_dataset)\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    \n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for e in range(epoch):\n",
    "        for i, (data) in enumerate(tr_loader):\n",
    "            padded_input, input_lengths, padded_target, target_lengths= data\n",
    "            padded_input = padded_input.cuda()\n",
    "            input_lengths = input_lengths.cuda()\n",
    "            padded_target = padded_target.cuda()\n",
    "            target_lengths = target_lengths.cuda()\n",
    "            loss = model(padded_input, input_lengths, padded_target, target_lengths)\n",
    "    #         print(loss) #.requires_grad\n",
    "            print_loss_total += float(loss)\n",
    "            plot_loss_total += float(loss)\n",
    "\n",
    "            optimizier.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            optimizier.step()\n",
    "\n",
    "            if (i+1) % print_every == 0:\n",
    "                print_loss_avg = print_loss_total / print_every\n",
    "                print_loss_total = 0\n",
    "                txt = 'Epoch %d | Iter %d | %s (%d %d%%) %.4f' % (e+1, i+1, timeSince(start, (e *n_iters +i+1) / (n_iters*epoch)),\n",
    "                                             (i+1), (e *n_iters +i+1) / (n_iters*epoch) * 100, print_loss_avg)\n",
    "                print(txt)\n",
    "\n",
    "            if i+1 % plot_every == 0:\n",
    "                plot_loss_avg = plot_loss_total / plot_every\n",
    "                plot_losses.append(plot_loss_avg)\n",
    "                plot_loss_total = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CTC_Model(\n",
      "  (encoder): Encoder(\n",
      "    (rnn): LSTM(83, 256, batch_first=True, bidirectional=True)\n",
      "  )\n",
      "  (ctc_loss): CTCLoss()\n",
      ")\n",
      "Epoch 1 | Iter 100 | 0m 12s (- 183m 53s) (100 0%) 275.1038\n",
      "Epoch 1 | Iter 200 | 0m 22s (- 163m 38s) (200 0%) 224.6259\n",
      "Epoch 1 | Iter 300 | 0m 35s (- 170m 25s) (300 0%) 213.2085\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "cuDNN error: CUDNN_STATUS_EXECUTION_FAILED",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-e05232f92943>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m#                                      momentum=momentum,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m                                      weight_decay=l2)\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mtrainIters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-36-01301d7752c9>\u001b[0m in \u001b[0;36mtrainIters\u001b[0;34m(model, epoch, optimizier, print_every, plot_every, learning_rate)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0moptimizier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0moptimizier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cuDNN error: CUDNN_STATUS_EXECUTION_FAILED"
     ]
    }
   ],
   "source": [
    "input_size = 83\n",
    "\n",
    "hidden_size = 256\n",
    "vocab_size = len(char_list)\n",
    "embedding_dim = 512\n",
    "sos_id = 0\n",
    "eos_id = 1\n",
    "learning_rate = 1e-3\n",
    "momentum = 0\n",
    "l2 = 1e-5\n",
    "\n",
    "IGNORE_ID=-1\n",
    "\n",
    "\n",
    "model = CTC_Model(input_size, hidden_size)\n",
    "print(model)\n",
    "model.cuda()\n",
    "\n",
    "optimizier = torch.optim.Adam(model.parameters(),\n",
    "                                     lr=learning_rate,\n",
    "#                                      momentum=momentum,\n",
    "                                     weight_decay=l2)\n",
    "trainIters(model, 1,optimizier, print_every=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
