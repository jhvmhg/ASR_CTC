{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from pathlib import Path\n",
    "import kaldiio\n",
    "import sys\n",
    "import gc\n",
    "import json\n",
    "import time\n",
    "from lm_utils import load_dataset, ParallelSentenceIterator\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "print_use = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\"\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label=\"/home1/meichaoyang/workspace/git/espnet/egs/aishell2/asr2/data/local/lm_train/train.txt\"\n",
    "valid_label=\"/home1/meichaoyang/workspace/git/espnet/egs/aishell2/asr2/data/local/lm_train/valid.txt\"\n",
    "dict_path=\"/home1/meichaoyang/workspace/git/espnet/egs/aishell2/asr2/data/lang_1char/train_sp_units.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 读取词典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(dict_path, \"rb\") as f:\n",
    "    dictionary = f.readlines()\n",
    "char_list = [entry.decode(\"utf-8\").split(\" \")[0] for entry in dictionary]\n",
    "char_list.insert(0, \"<blank>\")\n",
    "char_list.append(\"<eos>\")\n",
    "char_list_dict = {x: i for i, x in enumerate(char_list)}\n",
    "n_vocab = len(char_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "933907it [00:07, 128065.04it/s]\n"
     ]
    }
   ],
   "source": [
    "train, n_train_tokens, n_train_oovs = load_dataset(train_label, char_list_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50492it [00:00, 134685.67it/s]\n"
     ]
    }
   ],
   "source": [
    "val, n_val_tokens, n_val_oovs = load_dataset(valid_label, char_list_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen=100\n",
    "batch_size=32\n",
    "unk = char_list_dict[\"<unk>\"]\n",
    "eos = char_list_dict[\"<eos>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = ParallelSentenceIterator(\n",
    "        train,\n",
    "        batch_size,\n",
    "        max_length=maxlen,\n",
    "        sos=eos,\n",
    "        eos=eos,\n",
    "        shuffle=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_iter = ParallelSentenceIterator(\n",
    "        train,\n",
    "        batch_size,\n",
    "        max_length=maxlen,\n",
    "        sos=eos,\n",
    "        eos=eos,\n",
    "        shuffle=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_LM(nn.Module):\n",
    "    def __init__(self, n_vocab, n_layers, n_units, n_embed=None, dropout_rate=0.5):\n",
    "        nn.Module.__init__(self)\n",
    "        if n_embed is None:\n",
    "            n_embed = n_units\n",
    "        self.embed = nn.Embedding(n_vocab, n_embed)\n",
    "        self.rnn = nn.ModuleList(\n",
    "                [nn.LSTMCell(n_embed, n_units)]\n",
    "                + [nn.LSTMCell(n_units, n_units) for _ in range(n_layers - 1)]\n",
    "            )\n",
    "        self.lo = nn.Linear(n_units, n_vocab)\n",
    "        self.n_layers = n_layers\n",
    "        self.n_units = n_units\n",
    "        self.dropout = nn.ModuleList(\n",
    "            [nn.Dropout(dropout_rate) for _ in range(n_layers + 1)]\n",
    "        )\n",
    "\n",
    "    def zero_state(self, batchsize):\n",
    "        \"\"\"Initialize state.\"\"\"\n",
    "        p = next(self.parameters())\n",
    "        return torch.zeros(batchsize, self.n_units).to(device=p.device, dtype=p.dtype)\n",
    "    \n",
    "    \n",
    "    def forward(self, state, x):\n",
    "        \"\"\"Forward neural networks.\"\"\"\n",
    "        if state is None:\n",
    "            h = [\n",
    "                self.zero_state(x.size(0)).to(device)\n",
    "                for n in range(self.n_layers)\n",
    "            ]\n",
    "            state = {\"h\": h}\n",
    "            c = [\n",
    "                self.zero_state(x.size(0)).to(device)\n",
    "                for n in range(self.n_layers)\n",
    "            ]\n",
    "            state = {\"c\": c, \"h\": h}\n",
    "\n",
    "        h = [None] * self.n_layers\n",
    "        emb = self.embed(x)\n",
    "        c = [None] * self.n_layers\n",
    "        h[0], c[0] = self.rnn[0](\n",
    "            self.dropout[0](emb), (state[\"h\"][0], state[\"c\"][0])\n",
    "        )\n",
    "        for n in range(1, self.n_layers):\n",
    "            h[n], c[n] = self.rnn[n](\n",
    "                self.dropout[n](h[n - 1]), (state[\"h\"][n], state[\"c\"][n])\n",
    "            )\n",
    "        state = {\"c\": c, \"h\": h}\n",
    "        \n",
    "        y = self.lo(self.dropout[-1](h[-1]))\n",
    "        return state, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifierWithState(nn.Module):\n",
    "    \"\"\"A wrapper for pytorch RNNLM.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, rnnlm, lossfun=nn.CrossEntropyLoss(reduction=\"none\"), label_key=-1\n",
    "    ):\n",
    "        \"\"\"Initialize class.\n",
    "\n",
    "        :param torch.nn.Module predictor : The RNNLM\n",
    "        :param function lossfun : The loss function to use\n",
    "        :param int/str label_key :\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        super(ClassifierWithState, self).__init__()\n",
    "        self.lossfun = lossfun\n",
    "        self.y = None\n",
    "        self.loss = None\n",
    "        self.label_key = int(label_key)\n",
    "        self.predictor = rnnlm\n",
    "\n",
    "    def forward(self, state, *args, **kwargs):\n",
    "        \"\"\"Compute the loss value for an input and label pair.\n",
    "\n",
    "        Notes:\n",
    "            It also computes accuracy and stores it to the attribute.\n",
    "            When ``label_key`` is ``int``, the corresponding element in ``args``\n",
    "            is treated as ground truth labels. And when it is ``str``, the\n",
    "            element in ``kwargs`` is used.\n",
    "            The all elements of ``args`` and ``kwargs`` except the groundtruth\n",
    "            labels are features.\n",
    "            It feeds features to the predictor and compare the result\n",
    "            with ground truth labels.\n",
    "\n",
    "        :param torch.Tensor state : the LM state\n",
    "        :param list[torch.Tensor] args : Input minibatch\n",
    "        :param dict[torch.Tensor] kwargs : Input minibatch\n",
    "        :return loss value\n",
    "        :rtype torch.Tensor\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        if not (-len(args) <= self.label_key < len(args)):\n",
    "            msg = \"Label key %d is out of bounds\" % self.label_key\n",
    "            raise ValueError(msg)\n",
    "        t = args[self.label_key]\n",
    "        if self.label_key == -1:\n",
    "            args = args[:-1]\n",
    "        else:\n",
    "            args = args[: self.label_key] + args[self.label_key + 1 :]\n",
    "\n",
    "\n",
    "        self.y = None\n",
    "        self.loss = None\n",
    "        state, self.y = self.predictor(state, *args, **kwargs)\n",
    "        self.loss = self.lossfun(self.y, t)\n",
    "        return state, self.loss\n",
    "\n",
    "    def predict(self, state, x):\n",
    "        \"\"\"Predict log probabilities for given state and input x using the predictor.\n",
    "\n",
    "        :param torch.Tensor state : The current state\n",
    "        :param torch.Tensor x : The input\n",
    "        :return a tuple (new state, log prob vector)\n",
    "        :rtype (torch.Tensor, torch.Tensor)\n",
    "        \"\"\"\n",
    "        if hasattr(self.predictor, \"normalized\") and self.predictor.normalized:\n",
    "            return self.predictor(state, x)\n",
    "        else:\n",
    "            state, z = self.predictor(state, x)\n",
    "            return state, F.log_softmax(z, dim=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DefaultRNNLM(nn.Module):\n",
    "  \n",
    "\n",
    "    def __init__(self, n_vocab, layer, unit, embed_unit, dropout_rate=0.5):\n",
    "        \"\"\"Initialize class.\n",
    "\n",
    "        Args:\n",
    "            n_vocab (int): The size of the vocabulary\n",
    "\n",
    "        \"\"\"\n",
    "        nn.Module.__init__(self)\n",
    "\n",
    "        self.model = ClassifierWithState(\n",
    "            RNN_LM(n_vocab, layer, unit, embed_unit, dropout_rate)\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        \"\"\"Compute LM loss value from buffer sequences.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input ids. (batch, len)\n",
    "            t (torch.Tensor): Target ids. (batch, len)\n",
    "\n",
    "        Returns:\n",
    "            tuple[torch.Tensor, torch.Tensor, torch.Tensor]: Tuple of\n",
    "                loss to backward (scalar),\n",
    "                negative log-likelihood of t: -log p(t) (scalar) and\n",
    "                the number of elements in x (scalar)\n",
    "\n",
    "        Notes:\n",
    "            The last two return values are used\n",
    "            in perplexity: p(t)^{-n} = exp(-log p(t) / n)\n",
    "\n",
    "        \"\"\"\n",
    "        loss = 0\n",
    "        logp = 0\n",
    "        count = torch.tensor(0).long()\n",
    "        state = None\n",
    "        batch_size, sequence_length = x.shape\n",
    "        for i in range(sequence_length):\n",
    "            # Compute the loss at this time step and accumulate it\n",
    "            state, loss_batch = self.model(state, x[:, i], t[:, i])\n",
    "            non_zeros = torch.sum(x[:, i] != 0, dtype=loss_batch.dtype)\n",
    "            loss += loss_batch.mean() * non_zeros\n",
    "            logp += torch.sum(loss_batch * non_zeros)\n",
    "            count += int(non_zeros)\n",
    "        return loss / batch_size, loss, count.to(loss.device)\n",
    "\n",
    "    def score(self, y, state, x):\n",
    "        \"\"\"Score new token.\n",
    "\n",
    "        Args:\n",
    "            y (torch.Tensor): 1D torch.int64 prefix tokens.\n",
    "            state: Scorer state for prefix tokens\n",
    "            x (torch.Tensor): 2D encoder feature that generates ys.\n",
    "\n",
    "        Returns:\n",
    "            tuple[torch.Tensor, Any]: Tuple of\n",
    "                torch.float32 scores for next token (n_vocab)\n",
    "                and next state for ys\n",
    "\n",
    "        \"\"\"\n",
    "        new_state, scores = self.model.predict(state, y[-1].unsqueeze(0))\n",
    "        return scores.squeeze(0), new_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = 3\n",
    "unit = 256\n",
    "embed_unit = 200\n",
    "dropout_rate = 0.5\n",
    "\n",
    "model = DefaultRNNLM(n_vocab, layer, unit, embed_unit, dropout_rate)\n",
    "model.to(device) # 589MB\n",
    "\n",
    "optimizier = torch.optim.Adam(model.parameters(),\n",
    "                                     lr=1e-3,\n",
    "#                                      momentum=momentum,\n",
    "                                     weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter.iteration = 0\n",
    "train_iter.epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Iter 100 | 0m 5s (- 16868m 5s) (100 0%) 80.6223\n",
      "Epoch 1 | Iter 200 | 0m 10s (- 16930m 52s) (200 0%) 75.6375\n",
      "Epoch 1 | Iter 300 | 0m 15s (- 16237m 9s) (300 0%) 67.7000\n",
      "Epoch 1 | Iter 400 | 0m 21s (- 16555m 15s) (400 0%) 77.3295\n",
      "Epoch 1 | Iter 500 | 0m 26s (- 16595m 5s) (500 0%) 73.6129\n",
      "Epoch 1 | Iter 600 | 0m 31s (- 16575m 53s) (600 0%) 74.7071\n",
      "Epoch 1 | Iter 700 | 0m 37s (- 16600m 8s) (700 0%) 69.3573\n",
      "Epoch 1 | Iter 800 | 0m 42s (- 16587m 21s) (800 0%) 68.2111\n",
      "Epoch 1 | Iter 900 | 0m 48s (- 16726m 55s) (900 0%) 74.0991\n",
      "Epoch 1 | Iter 1000 | 0m 54s (- 16887m 29s) (1000 0%) 78.6547\n",
      "Epoch 1 | Iter 1100 | 0m 59s (- 16836m 8s) (1100 0%) 66.8761\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-77e005a846e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mplot_loss_total\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0moptimizier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0moptimizier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print_every = 100\n",
    "plot_every  = 100\n",
    "epoch=20\n",
    "\n",
    "start = time.time()\n",
    "n_iters = len(train)\n",
    "plot_losses = []\n",
    "print_loss_total = 0  # Reset every print_every\n",
    "plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "while train_iter.epoch < epoch:\n",
    "    e=train_iter.epoch\n",
    "    i=train_iter.iteration\n",
    "    \n",
    "    data = train_iter.__next__()\n",
    "    x = torch.tensor([i[0] for i in data]).to(device)\n",
    "    t = torch.tensor([i[1] for i in data]).to(device)\n",
    "    \n",
    "    loss, _, _ = model(x, t)\n",
    "    \n",
    "    print_loss_total += float(loss)\n",
    "    plot_loss_total += float(loss)\n",
    "    optimizier.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    optimizier.step()\n",
    "    \n",
    "    if (i+1) % print_every == 0:\n",
    "        print_loss_avg = print_loss_total / print_every\n",
    "        print_loss_total = 0\n",
    "        txt = 'Epoch %d | Iter %d | %s (%d %d%%) %.4f' % (e+1, i+1, timeSince(start, (e *n_iters +i+1) / (n_iters * epoch)),\n",
    "                                             (i+1), (train_iter.epoch *n_iters +i+1) / (n_iters*epoch) * 100, print_loss_avg)\n",
    "        print(txt)\n",
    "\n",
    "    if (i+1) % plot_every == 0:\n",
    "        plot_loss_avg = plot_loss_total / plot_every\n",
    "        plot_losses.append(plot_loss_avg)\n",
    "        plot_loss_total = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3288, 4235, 1661,  952, 1661], dtype=int32)"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
