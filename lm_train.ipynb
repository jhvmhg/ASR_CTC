{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from pathlib import Path\n",
    "import kaldiio\n",
    "import sys\n",
    "import gc\n",
    "import json\n",
    "import time\n",
    "from lm_utils import load_dataset, ParallelSentenceIterator\n",
    "from utils import pad_list\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "print_use = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\"\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label=\"/home1/meichaoyang/workspace/git/espnet/egs/aishell2/asr2/data/local/lm_train/train.txt\"\n",
    "valid_label=\"/home1/meichaoyang/workspace/git/espnet/egs/aishell2/asr2/data/local/lm_train/valid.txt\"\n",
    "dict_path=\"/home1/meichaoyang/workspace/git/espnet/egs/aishell2/asr2/data/lang_1char/train_sp_units.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 读取词典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(dict_path, \"rb\") as f:\n",
    "    dictionary = f.readlines()\n",
    "char_list = [entry.decode(\"utf-8\").split(\" \")[0] for entry in dictionary]\n",
    "char_list.insert(0, \"<blank>\")\n",
    "char_list.append(\"<sos>\")\n",
    "char_list.append(\"<eos>\")\n",
    "char_list_dict = {x: i for i, x in enumerate(char_list)}\n",
    "n_vocab = len(char_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "933907it [00:05, 178425.52it/s]\n"
     ]
    }
   ],
   "source": [
    "train, n_train_tokens, n_train_oovs = load_dataset(train_label, char_list_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50492it [00:00, 173165.77it/s]\n"
     ]
    }
   ],
   "source": [
    "val, n_val_tokens, n_val_oovs = load_dataset(valid_label, char_list_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen=100\n",
    "batch_size=32\n",
    "unk = char_list_dict[\"<unk>\"]\n",
    "sos = char_list_dict[\"<sos>\"]\n",
    "eos = char_list_dict[\"<eos>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = ParallelSentenceIterator(\n",
    "        train,\n",
    "        batch_size,\n",
    "        max_length=maxlen,\n",
    "        sos=sos,\n",
    "        eos=eos,\n",
    "        shuffle=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_iter = ParallelSentenceIterator(\n",
    "        train,\n",
    "        batch_size,\n",
    "        max_length=maxlen,\n",
    "        sos=sos,\n",
    "        eos=eos,\n",
    "        shuffle=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_LM(nn.Module):\n",
    "    def __init__(self, n_vocab, n_layers, n_units, n_embed=None, dropout_rate=0.5):\n",
    "        nn.Module.__init__(self)\n",
    "        if n_embed is None:\n",
    "            n_embed = n_units\n",
    "        self.embed = nn.Embedding(n_vocab, n_embed)\n",
    "        self.rnn = nn.ModuleList(\n",
    "                [nn.LSTMCell(n_embed, n_units)]\n",
    "                + [nn.LSTMCell(n_units, n_units) for _ in range(n_layers - 1)]\n",
    "            )\n",
    "        self.lo = nn.Linear(n_units, n_vocab)\n",
    "        self.n_layers = n_layers\n",
    "        self.n_units = n_units\n",
    "        self.dropout = nn.ModuleList(\n",
    "            [nn.Dropout(dropout_rate) for _ in range(n_layers + 1)]\n",
    "        )\n",
    "\n",
    "    def zero_state(self, batchsize):\n",
    "        \"\"\"Initialize state.\"\"\"\n",
    "        p = next(self.parameters())\n",
    "        return torch.zeros(batchsize, self.n_units).to(device=p.device, dtype=p.dtype)\n",
    "    \n",
    "    \n",
    "    def forward(self, state, x):\n",
    "        \"\"\"Forward neural networks.\"\"\"\n",
    "        if state is None:\n",
    "            h = [\n",
    "                self.zero_state(x.size(0)).to(device)\n",
    "                for n in range(self.n_layers)\n",
    "            ]\n",
    "            state = {\"h\": h}\n",
    "            c = [\n",
    "                self.zero_state(x.size(0)).to(device)\n",
    "                for n in range(self.n_layers)\n",
    "            ]\n",
    "            state = {\"c\": c, \"h\": h}\n",
    "\n",
    "        h = [None] * self.n_layers\n",
    "        emb = self.embed(x)\n",
    "        c = [None] * self.n_layers\n",
    "        h[0], c[0] = self.rnn[0](\n",
    "            self.dropout[0](emb), (state[\"h\"][0], state[\"c\"][0])\n",
    "        )\n",
    "        for n in range(1, self.n_layers):\n",
    "            h[n], c[n] = self.rnn[n](\n",
    "                self.dropout[n](h[n - 1]), (state[\"h\"][n], state[\"c\"][n])\n",
    "            )\n",
    "        state = {\"c\": c, \"h\": h}\n",
    "        \n",
    "        y = self.lo(self.dropout[-1](h[-1]))\n",
    "        return state, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifierWithState(nn.Module):\n",
    "    \"\"\"A wrapper for pytorch RNNLM.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, rnnlm, lossfun=nn.CrossEntropyLoss(reduction=\"none\"), label_key=-1\n",
    "    ):\n",
    "        \"\"\"Initialize class.\n",
    "\n",
    "        :param torch.nn.Module predictor : The RNNLM\n",
    "        :param function lossfun : The loss function to use\n",
    "        :param int/str label_key :\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        super(ClassifierWithState, self).__init__()\n",
    "        self.lossfun = lossfun\n",
    "        self.y = None\n",
    "        self.loss = None\n",
    "        self.label_key = int(label_key)\n",
    "        self.predictor = rnnlm\n",
    "\n",
    "    def forward(self, state, *args, **kwargs):\n",
    "        \"\"\"Compute the loss value for an input and label pair.\n",
    "\n",
    "        Notes:\n",
    "            It also computes accuracy and stores it to the attribute.\n",
    "            When ``label_key`` is ``int``, the corresponding element in ``args``\n",
    "            is treated as ground truth labels. And when it is ``str``, the\n",
    "            element in ``kwargs`` is used.\n",
    "            The all elements of ``args`` and ``kwargs`` except the groundtruth\n",
    "            labels are features.\n",
    "            It feeds features to the predictor and compare the result\n",
    "            with ground truth labels.\n",
    "\n",
    "        :param torch.Tensor state : the LM state\n",
    "        :param list[torch.Tensor] args : Input minibatch\n",
    "        :param dict[torch.Tensor] kwargs : Input minibatch\n",
    "        :return loss value\n",
    "        :rtype torch.Tensor\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        if not (-len(args) <= self.label_key < len(args)):\n",
    "            msg = \"Label key %d is out of bounds\" % self.label_key\n",
    "            raise ValueError(msg)\n",
    "        t = args[self.label_key]\n",
    "        if self.label_key == -1:\n",
    "            args = args[:-1]\n",
    "        else:\n",
    "            args = args[: self.label_key] + args[self.label_key + 1 :]\n",
    "\n",
    "\n",
    "        self.y = None\n",
    "        self.loss = None\n",
    "        state, self.y = self.predictor(state, *args, **kwargs)\n",
    "        self.loss = self.lossfun(self.y, t)\n",
    "        return state, self.loss\n",
    "\n",
    "    def predict(self, state, x):\n",
    "        \"\"\"Predict log probabilities for given state and input x using the predictor.\n",
    "\n",
    "        :param torch.Tensor state : The current state\n",
    "        :param torch.Tensor x : The input\n",
    "        :return a tuple (new state, log prob vector)\n",
    "        :rtype (torch.Tensor, torch.Tensor)\n",
    "        \"\"\"\n",
    "        if hasattr(self.predictor, \"normalized\") and self.predictor.normalized:\n",
    "            return self.predictor(state, x)\n",
    "        else:\n",
    "            state, z = self.predictor(state, x)\n",
    "            return state, F.log_softmax(z, dim=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DefaultRNNLM(nn.Module):\n",
    "  \n",
    "\n",
    "    def __init__(self, n_vocab, layer, unit, embed_unit, dropout_rate=0.5):\n",
    "        \"\"\"Initialize class.\n",
    "\n",
    "        Args:\n",
    "            n_vocab (int): The size of the vocabulary\n",
    "\n",
    "        \"\"\"\n",
    "        nn.Module.__init__(self)\n",
    "\n",
    "        self.model = ClassifierWithState(\n",
    "            RNN_LM(n_vocab, layer, unit, embed_unit, dropout_rate)\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        \"\"\"Compute LM loss value from buffer sequences.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input ids. (batch, len)\n",
    "            t (torch.Tensor): Target ids. (batch, len)\n",
    "\n",
    "        Returns:\n",
    "            tuple[torch.Tensor, torch.Tensor, torch.Tensor]: Tuple of\n",
    "                loss to backward (scalar),\n",
    "                negative log-likelihood of t: -log p(t) (scalar) and\n",
    "                the number of elements in x (scalar)\n",
    "\n",
    "        Notes:\n",
    "            The last two return values are used\n",
    "            in perplexity: p(t)^{-n} = exp(-log p(t) / n)\n",
    "\n",
    "        \"\"\"\n",
    "        loss = 0\n",
    "        logp = 0\n",
    "        count = torch.tensor(0).long()\n",
    "        state = None\n",
    "        batch_size, sequence_length = x.shape\n",
    "        for i in range(sequence_length):\n",
    "            # Compute the loss at this time step and accumulate it\n",
    "            state, loss_batch = self.model(state, x[:, i], t[:, i])\n",
    "            non_zeros = torch.sum(x[:, i] != 0, dtype=loss_batch.dtype)\n",
    "            loss += loss_batch.mean() * non_zeros\n",
    "            logp += torch.sum(loss_batch * non_zeros)\n",
    "            count += int(non_zeros)\n",
    "        return loss / batch_size, loss, count.to(loss.device)\n",
    "\n",
    "    def score(self, y, state, x):\n",
    "        \"\"\"Score new token.\n",
    "\n",
    "        Args:\n",
    "            y (torch.Tensor): 1D torch.int64 prefix tokens.\n",
    "            state: Scorer state for prefix tokens\n",
    "            x (torch.Tensor): 2D encoder feature that generates ys.\n",
    "\n",
    "        Returns:\n",
    "            tuple[torch.Tensor, Any]: Tuple of\n",
    "                torch.float32 scores for next token (n_vocab)\n",
    "                and next state for ys\n",
    "\n",
    "        \"\"\"\n",
    "        new_state, scores = self.model.predict(state, y[-1].unsqueeze(0))\n",
    "        return scores.squeeze(0), new_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model-未使用数据pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = 3\n",
    "unit = 256\n",
    "embed_unit = 200\n",
    "dropout_rate = 0.5\n",
    "\n",
    "model = DefaultRNNLM(n_vocab, layer, unit, embed_unit, dropout_rate)\n",
    "model.to(device) # 589MB\n",
    "\n",
    "optimizier = torch.optim.Adam(model.parameters(),\n",
    "                                     lr=1e-3,\n",
    "#                                      momentum=momentum,\n",
    "                                     weight_decay=1e-5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 重置train_iter\n",
    "train_iter.iteration = 0\n",
    "train_iter.epoch = 0\n",
    "\n",
    "\n",
    "print_every = 100\n",
    "plot_every  = 100\n",
    "epoch=20\n",
    "\n",
    "start = time.time()\n",
    "n_iters =len(train_iter.batch_indices)\n",
    "plot_losses = []\n",
    "print_loss_total = 0  # Reset every print_every\n",
    "plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "while train_iter.epoch < epoch:\n",
    "    e=train_iter.epoch\n",
    "    i=train_iter.iteration\n",
    "    \n",
    "    data = train_iter.__next__()\n",
    "    try:\n",
    "        x = torch.tensor([i[0] for i in data]).to(device)\n",
    "        t = torch.tensor([i[1] for i in data]).to(device)\n",
    "    except:\n",
    "        print(\"异常,train_iter.iteration:\",train_iter.iteration)\n",
    "        continue\n",
    "    loss, _, _ = model(x, t)\n",
    "    \n",
    "    print_loss_total += float(loss)\n",
    "    plot_loss_total += float(loss)\n",
    "    optimizier.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    optimizier.step()\n",
    "    \n",
    "    if (i+1) % print_every == 0:\n",
    "        print_loss_avg = print_loss_total / print_every\n",
    "        print_loss_total = 0\n",
    "        txt = 'Epoch %d | Iter %d | %s (%d %d%%) %.4f' % (e+1, i+1, timeSince(start, (e *n_iters +i+1) / (n_iters * epoch)),\n",
    "                                             (i+1), (train_iter.epoch *n_iters +i+1) / (n_iters*epoch) * 100, print_loss_avg)\n",
    "        print(txt)\n",
    "\n",
    "    if (i+1) % plot_every == 0:\n",
    "        plot_loss_avg = plot_loss_total / plot_every\n",
    "        plot_losses.append(plot_loss_avg)\n",
    "        plot_loss_total = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model,\"model.mdl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用数据pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = 3\n",
    "unit = 256\n",
    "embed_unit = 200\n",
    "dropout_rate = 0.5\n",
    "\n",
    "model1 = DefaultRNNLM(n_vocab, layer, unit, embed_unit, dropout_rate)\n",
    "model1.to(device) # 589MB\n",
    "\n",
    "optimizier1 = torch.optim.Adam(model1.parameters(),\n",
    "                                     lr=1e-3,\n",
    "#                                      momentum=momentum,\n",
    "                                     weight_decay=1e-5)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Iter 100 | 0m 5s (- 529m 57s) (100 0%) 68.0919\n",
      "Epoch 1 | Iter 200 | 0m 11s (- 550m 13s) (200 0%) 72.9332\n",
      "Epoch 1 | Iter 300 | 0m 16s (- 542m 40s) (300 0%) 68.9128\n",
      "Epoch 1 | Iter 400 | 0m 22s (- 541m 13s) (400 0%) 70.7360\n",
      "Epoch 1 | Iter 500 | 0m 27s (- 535m 12s) (500 0%) 66.4887\n",
      "Epoch 1 | Iter 600 | 0m 32s (- 531m 50s) (600 0%) 65.1339\n",
      "Epoch 1 | Iter 700 | 0m 37s (- 527m 26s) (700 0%) 62.1745\n",
      "Epoch 1 | Iter 800 | 0m 43s (- 528m 16s) (800 0%) 67.7849\n",
      "Epoch 1 | Iter 900 | 0m 48s (- 525m 28s) (900 0%) 62.2450\n",
      "Epoch 1 | Iter 1000 | 0m 53s (- 522m 54s) (1000 0%) 62.1213\n",
      "Epoch 1 | Iter 1100 | 0m 59s (- 527m 13s) (1100 0%) 67.5676\n",
      "Epoch 1 | Iter 1200 | 1m 5s (- 530m 4s) (1200 0%) 64.8291\n",
      "Epoch 1 | Iter 1300 | 1m 10s (- 528m 26s) (1300 0%) 68.3907\n",
      "Epoch 1 | Iter 1400 | 1m 15s (- 525m 44s) (1400 0%) 61.9682\n",
      "Epoch 1 | Iter 1500 | 1m 21s (- 524m 33s) (1500 0%) 62.4567\n",
      "Epoch 1 | Iter 1600 | 1m 26s (- 525m 50s) (1600 0%) 64.8444\n",
      "Epoch 1 | Iter 1700 | 1m 32s (- 526m 33s) (1700 0%) 65.8358\n",
      "Epoch 1 | Iter 1800 | 1m 37s (- 527m 11s) (1800 0%) 65.3272\n",
      "Epoch 1 | Iter 1900 | 1m 43s (- 526m 37s) (1900 0%) 59.8486\n",
      "Epoch 1 | Iter 2000 | 1m 49s (- 528m 27s) (2000 0%) 64.5866\n",
      "Epoch 1 | Iter 2100 | 1m 54s (- 526m 29s) (2100 0%) 67.0032\n",
      "Epoch 1 | Iter 2200 | 1m 59s (- 525m 54s) (2200 0%) 61.2832\n",
      "Epoch 1 | Iter 2300 | 2m 4s (- 526m 33s) (2300 0%) 67.4238\n",
      "Epoch 1 | Iter 2400 | 2m 10s (- 527m 54s) (2400 0%) 69.8303\n",
      "Epoch 1 | Iter 2500 | 2m 16s (- 528m 7s) (2500 0%) 63.3895\n",
      "Epoch 1 | Iter 2600 | 2m 21s (- 527m 17s) (2600 0%) 60.8195\n",
      "Epoch 1 | Iter 2700 | 2m 27s (- 527m 56s) (2700 0%) 68.2156\n",
      "Epoch 1 | Iter 2800 | 2m 32s (- 528m 52s) (2800 0%) 68.8887\n",
      "Epoch 1 | Iter 2900 | 2m 38s (- 530m 7s) (2900 0%) 63.7931\n",
      "Epoch 1 | Iter 3000 | 2m 43s (- 528m 55s) (3000 0%) 64.5442\n",
      "Epoch 1 | Iter 3100 | 2m 49s (- 528m 26s) (3100 0%) 61.9157\n",
      "Epoch 1 | Iter 3200 | 2m 54s (- 527m 25s) (3200 0%) 58.5556\n",
      "Epoch 1 | Iter 3300 | 2m 59s (- 527m 20s) (3300 0%) 63.5533\n",
      "Epoch 1 | Iter 3400 | 3m 4s (- 524m 47s) (3400 0%) 61.7622\n",
      "Epoch 1 | Iter 3500 | 3m 9s (- 522m 32s) (3500 0%) 63.7509\n",
      "Epoch 1 | Iter 3600 | 3m 13s (- 519m 17s) (3600 0%) 65.2545\n",
      "Epoch 1 | Iter 3700 | 3m 17s (- 516m 0s) (3700 0%) 59.9540\n"
     ]
    }
   ],
   "source": [
    "# 重置train_iter\n",
    "train_iter.iteration = 0\n",
    "train_iter.epoch = 0\n",
    "\n",
    "print_every = 100\n",
    "plot_every  = 100\n",
    "epoch=20\n",
    "\n",
    "start = time.time()\n",
    "n_iters =len(train_iter.batch_indices)\n",
    "plot_losses = []\n",
    "print_loss_total = 0  # Reset every print_every\n",
    "plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "while train_iter.epoch < epoch:\n",
    "    e=train_iter.epoch\n",
    "    i=train_iter.iteration\n",
    "    \n",
    "    data = train_iter.__next__()\n",
    "    x = pad_list([torch.from_numpy(i[0]) for i in data], char_list_dict[\"<eos>\"]).to(device)\n",
    "    t = pad_list([torch.from_numpy(i[1]) for i in data], char_list_dict[\"<eos>\"]).to(device)\n",
    "\n",
    "    loss, _, _ = model1(x, t)\n",
    "    \n",
    "    print_loss_total += float(loss)\n",
    "    plot_loss_total += float(loss)\n",
    "    optimizier1.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    optimizier1.step()\n",
    "    \n",
    "    if (i+1) % print_every == 0:\n",
    "        print_loss_avg = print_loss_total / print_every\n",
    "        print_loss_total = 0\n",
    "        txt = 'Epoch %d | Iter %d | %s (%d %d%%) %.4f' % (e+1, i+1, timeSince(start, (e *n_iters +i+1) / (n_iters * epoch)),\n",
    "                                             (i+1), (train_iter.epoch *n_iters +i+1) / (n_iters*epoch) * 100, print_loss_avg)\n",
    "        print(txt)\n",
    "\n",
    "    if (i+1) % plot_every == 0:\n",
    "        plot_loss_avg = plot_loss_total / plot_every\n",
    "        plot_losses.append(plot_loss_avg)\n",
    "        plot_loss_total = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model1,\"model1.mdl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
